{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An attention function can be described as\n",
    "```\n",
    "mapping a query and a set of key-value pairs to an output\n",
    "```\n",
    "**output** computed as a weighted sum of the values  \n",
    "**weight** compatibility between query and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled Dot-Product Attention from Attention is all you need\n",
    "\n",
    "$\n",
    "attention = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$\n",
    "\n",
    "1. matmul  U = Q @ K^T\n",
    "2. scale   U = U / sqrt(d_k)\n",
    "3. mask    U = U.masked_fill(mask, -inf)\n",
    "4. softmax A = softmax(U)\n",
    "5. matmul  O = A @ V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\" Scaled Dot-Product Attention \n",
    "    mapping a query and a set of key-value pairs to an output\n",
    "\n",
    "    input:\n",
    "        query  [B, n_q, d_q]\n",
    "        key    [B, n_k, d_k]\n",
    "        value  [B, n_v, d_v]\n",
    "    output:\n",
    "        attn   [B, n_q, n_k]\n",
    "        output [B, n_q, d_v]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "        # compute a weight vector for every query\n",
    "        # therefore apply softmax in key dimention\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        u = torch.bmm(q, k.transpose(1, 2))\n",
    "        # scale是为了防止数值过大从而导致softmax时梯度很小\n",
    "        u /= self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            u = u.masked_fill(mask, -np.inf)\n",
    "        \n",
    "        attn = self.softmax(u)\n",
    "        output = torch.bmm(attn, v)\n",
    "        \n",
    "        return attn, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试代码\n",
    "batch = 1\n",
    "n_q, n_k, n_v = 2, 4, 4\n",
    "d_q, d_k, d_v = 128, 128, 64\n",
    "\n",
    "q = torch.randn(batch, n_q, d_q)\n",
    "k = torch.randn(batch, n_k, d_k)\n",
    "v = torch.randn(batch, n_v, d_v)\n",
    "mask = torch.zeros(batch, n_q, n_k).bool()\n",
    "\n",
    "attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
    "attn, output = attention(q, k, v, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4])\n",
      "torch.Size([1, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "print(attn.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3740, 0.1008, 0.0738, 0.4514],\n",
      "         [0.6360, 0.2082, 0.0599, 0.0959]]])\n",
      "tensor([[[-0.2677, -0.1240, -1.0508, -0.2012,  0.4684, -0.9366,  0.0795,\n",
      "          -0.0091, -0.1899, -0.3730, -0.9943, -0.8041,  0.1018, -0.2556,\n",
      "          -1.4128,  0.7581, -0.6784, -0.1562, -0.1802,  0.2760, -0.0950,\n",
      "           1.0549,  0.3989, -0.2095, -0.6312, -0.9333, -0.4323,  0.2695,\n",
      "          -0.3454, -0.3770, -0.6076,  0.2579, -0.0761,  0.2060, -0.7542,\n",
      "           0.9421, -0.4471, -0.4111, -0.4425, -0.9084, -0.1988, -0.5302,\n",
      "           0.0885, -0.7709, -1.0963, -0.9873,  0.8236,  0.5818,  0.8382,\n",
      "           1.0121,  0.2256,  0.3858,  0.0234, -0.2685,  0.1367,  0.3838,\n",
      "           0.3765,  0.5326,  0.5310, -0.7640, -0.7884,  0.2384,  0.1531,\n",
      "          -0.2706],\n",
      "         [-0.0314, -0.0598, -1.2965,  0.2059, -0.1421, -0.4629,  0.0685,\n",
      "          -0.4133, -0.5710, -0.0967, -1.6474, -0.6449,  0.3460,  0.1037,\n",
      "          -1.0440,  0.5452, -0.2436, -0.6906,  0.2568,  0.1788, -0.6554,\n",
      "           1.7299,  0.0947,  0.1007, -0.6357,  0.3432, -0.8377,  0.2689,\n",
      "          -0.5823, -0.6854, -0.3525, -0.0386,  0.2427,  0.0196, -0.8857,\n",
      "           0.1665, -1.0873, -0.7280, -0.5345, -0.2923, -0.1958, -0.4011,\n",
      "           0.0512, -0.9417, -0.2656, -0.1620,  1.0449, -0.2067,  0.6292,\n",
      "           1.0127,  0.4825,  0.3631,  0.5192, -0.5595,  0.4604,  1.0028,\n",
      "          -0.0563,  0.4101,  1.3962, -0.2213, -0.6544, -0.0265, -0.3324,\n",
      "          -0.0459]]])\n"
     ]
    }
   ],
   "source": [
    "print(attn)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**变形金刚结构图如下所示，我们这里主要考虑decoder的结构。**  \n",
    "![](../Source/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**输入的格式**\n",
    "一句话len个单词，每个单词编码成为一个n维向量称为hidden_dim\n",
    "```python\n",
    "    Q: [B, len_q, dim_q]\n",
    "    K: [B, len_k, dim_k]\n",
    "    V: [B, len_v, dim_v]\n",
    "```\n",
    "\n",
    "注意力机制就是计算query和key的相关性矩阵，这就要求向量维度一致\n",
    "```python\n",
    "    assert dim_q == dim_k\n",
    "    attn_mask: [len_q, len_k]\n",
    "```\n",
    "softmax之后得到注意力权重，与value求出加权和即可，所以这里key的个数必须和value一致\n",
    "```python\n",
    "    assert len_k == len_v\n",
    "    output: [B, len_q, dim_v]\n",
    "```\n",
    "如果我们根据作者的抽象理解，最终的输出就是query数量个的value向量\n",
    "一般情况下我们认为这些维度都是相等的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim=768, nums_heads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_Q = nn.Linear(dim, d_q)\n",
    "        self.W_K = nn.Linear(dim, d_k)\n",
    "        self.W_V = nn.Linear(dim, d_v)\n",
    "        self.fc = nn.Linear(d_v, dim)\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "\n",
    "        self.nums_heads = nums_heads\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        residual, batch = q, q.shape(0)\n",
    "\n",
    "        q = self.W_Q(q).reshape(batch, n_q, self.nums_heads, -1).transpose(1, 2)\n",
    "        k = self.W_K(k).reshape(batch, n_k, self.nums_heads, -1).transpose(1, 2)\n",
    "        V = self.W_K(v).reshape(batch, n_v, self.nums_heads, -1).transpose(1, 2)\n",
    "\n",
    "        attn = torch.bmm(q, k.transpose(2, 3))  # [bacth, heads, n_q, n_k]\n",
    "        attn = F.softmax(attn / np.sqrt(d_k), dim=-1)\n",
    "        output = torch.bmm(attn, v).transpose(1, 2).reshape(batch, n_v, -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return self.ln(output + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim * 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.fc(x)\n",
    "\n",
    "        return self.ln(output + residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batchnorm是在一个batch的各个样本之间做归一化，参数大小是 2×C  \n",
    "Layernorm则是在特征之间做归一化，所以参数大小和batchsize有关？\n",
    "\n",
    "可以看看这个，LayerNorm就是加在最后D个维度上，被称为normalized_shape  \n",
    "https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
    "\n",
    "为什么对于图像是`nn.LayerNorm([C, H, W])`而对于NLP是`nn.LayerNorm(embedding_dim)`  \n",
    "考虑单独的实例，一张图像的组成就是[C, H, W]， 而一个单词只是[N]维的向量，这里做LN是在单词维度上，而不是句子，所以只需要最后一个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-12) -> None:\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.parameter(torch.zeros(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 是把batch内每个实例，单独对其特征求均值和方差，这里的特征只有最后一个维度\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True)\n",
    "\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)  # 尺度不变\n",
    "        out = self.gamma * out + self.beta  # 特征变换 所以gamma和beta是dim尺寸\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 5, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.5243e-01,  5.1689e-01,  8.7616e-01,  1.0935e+00,  1.0197e+00,\n",
       "          -6.1018e-02,  3.2073e-01, -1.6677e+00, -1.7136e+00, -8.3704e-01],\n",
       "         [ 3.4844e-01,  1.9638e-01, -8.7768e-01,  9.0137e-01, -1.2342e+00,\n",
       "          -1.3312e+00, -8.8244e-01,  1.5841e+00,  1.2937e+00,  1.4217e-03],\n",
       "         [ 1.6180e+00, -3.2185e-01, -1.2941e+00,  1.2915e+00,  5.7974e-01,\n",
       "          -1.1048e+00, -1.0464e+00, -4.7261e-01,  9.9943e-01, -2.4889e-01],\n",
       "         [-1.1997e+00,  9.4407e-01,  1.3663e+00,  5.6440e-01, -4.6944e-01,\n",
       "          -6.2003e-01,  1.6438e+00, -9.0377e-01, -1.1536e+00, -1.7216e-01],\n",
       "         [ 5.0406e-04,  1.3974e-01,  9.7277e-01, -1.9937e+00, -1.0500e+00,\n",
       "           1.1876e+00,  9.5969e-01,  3.2783e-01, -1.0999e+00,  5.5541e-01]],\n",
       "\n",
       "        [[-6.3729e-01, -6.3336e-01, -1.3643e+00,  1.0272e-01,  1.2369e+00,\n",
       "           1.7993e+00, -1.3663e+00,  8.2434e-01, -3.7042e-02,  7.5135e-02],\n",
       "         [-2.3591e-01,  1.4860e-01, -2.0166e+00, -5.7666e-01, -4.4758e-01,\n",
       "           1.9476e-02,  7.7409e-01,  7.9467e-01, -4.3541e-01,  1.9753e+00],\n",
       "         [-1.1713e+00,  2.9455e-01, -1.1608e+00, -1.0539e+00,  1.9371e+00,\n",
       "           4.5430e-01,  8.1215e-01,  7.2449e-01,  1.2483e-01, -9.6134e-01],\n",
       "         [ 2.0988e+00, -4.8998e-04, -1.1748e+00,  1.0260e+00, -5.6440e-01,\n",
       "           9.5688e-02, -7.6598e-01, -1.3228e+00,  6.9986e-01, -9.1896e-02],\n",
       "         [-2.0939e+00,  3.8122e-01, -9.4584e-01,  1.1204e+00,  2.9743e-01,\n",
       "           1.9118e-02,  1.6059e-01, -2.2655e-01,  1.7221e+00, -4.3456e-01]],\n",
       "\n",
       "        [[ 1.2550e+00,  5.9579e-02, -1.0254e-01,  2.6331e-01, -2.0532e-01,\n",
       "          -1.3366e+00, -1.8654e+00, -3.7761e-01,  8.2017e-01,  1.4893e+00],\n",
       "         [ 5.0822e-01, -1.1311e-01, -9.2192e-02, -1.4303e+00, -4.2565e-02,\n",
       "          -1.2573e+00,  1.7210e+00, -2.7725e-01,  1.6253e+00, -6.4177e-01],\n",
       "         [ 3.5256e-01,  1.0912e+00,  1.1232e+00,  7.7680e-01, -2.2446e-01,\n",
       "          -2.4630e+00,  7.5314e-02, -5.4920e-01,  3.4428e-01, -5.2662e-01],\n",
       "         [-1.0704e+00,  4.5800e-01,  4.4016e-01,  5.7000e-03,  1.3620e+00,\n",
       "           1.5770e+00, -1.0784e+00,  5.3291e-01, -1.4152e+00, -8.1181e-01],\n",
       "         [-1.4670e+00,  2.3350e-02, -8.6393e-01,  7.1089e-01, -4.5779e-01,\n",
       "          -9.6460e-01,  4.3071e-01, -4.3782e-01,  1.0135e+00,  2.0127e+00]],\n",
       "\n",
       "        [[-3.2657e-02,  4.7982e-01,  2.2310e-02,  4.8177e-01, -1.1954e+00,\n",
       "           2.2151e-01,  3.5954e-02,  2.0239e+00, -4.7760e-02, -1.9894e+00],\n",
       "         [-1.0335e+00,  7.6173e-01,  1.8587e-01, -2.1245e+00,  1.4659e+00,\n",
       "          -3.0880e-01,  1.1732e+00, -3.6972e-03, -3.5480e-01,  2.3862e-01],\n",
       "         [ 8.9813e-01, -1.4596e+00, -2.7746e-01, -5.0772e-01, -3.9375e-01,\n",
       "           1.4537e-01, -1.1358e+00,  6.6025e-01,  2.1934e+00, -1.2277e-01],\n",
       "         [ 6.8565e-01, -1.0356e-01, -1.0670e-01, -1.0574e+00, -8.3442e-01,\n",
       "          -1.0753e+00,  8.6892e-02,  4.9647e-01, -5.3955e-01,  2.4479e+00],\n",
       "         [ 3.0067e-01,  1.0862e+00, -1.9207e+00, -1.2832e-01, -1.7970e-01,\n",
       "           1.0862e+00,  4.6617e-01,  1.2726e+00, -9.2512e-01, -1.0580e+00]],\n",
       "\n",
       "        [[ 1.4016e-01, -5.2576e-01,  7.3827e-01, -4.2104e-01,  2.2885e+00,\n",
       "          -7.9916e-01,  4.0435e-01, -6.2540e-01, -1.5568e+00,  3.5682e-01],\n",
       "         [-6.4742e-01,  5.4117e-01, -9.5410e-01, -4.0044e-01,  8.2696e-02,\n",
       "          -1.4858e+00,  2.7313e-01, -5.1572e-01,  9.0636e-01,  2.2002e+00],\n",
       "         [-1.0228e+00,  3.8115e-01, -1.4697e+00,  3.6613e-02,  1.5507e-01,\n",
       "           4.7395e-01, -3.9602e-01,  2.1855e+00, -1.0105e+00,  6.6671e-01],\n",
       "         [-5.4334e-01, -8.0779e-01,  6.6213e-01, -3.3970e-03,  1.6001e+00,\n",
       "          -2.2160e-01,  1.3703e+00, -1.8004e+00,  5.2489e-01, -7.8089e-01],\n",
       "         [-1.8919e+00,  4.0881e-01,  1.2075e+00, -3.5861e-01,  7.7039e-01,\n",
       "          -6.6246e-01,  1.2683e+00, -1.2966e+00, -3.2005e-02,  5.8644e-01]],\n",
       "\n",
       "        [[-1.0693e+00,  6.8991e-01,  2.0645e-02,  8.8356e-01,  5.8630e-01,\n",
       "           5.0109e-01, -2.3144e+00,  1.0592e+00, -6.5942e-01,  3.0247e-01],\n",
       "         [ 1.4739e-01,  1.0917e+00,  3.8742e-01,  1.2500e+00, -1.2227e+00,\n",
       "          -1.9778e+00,  7.0401e-01, -7.0343e-01, -3.9685e-01,  7.2028e-01],\n",
       "         [ 1.6892e+00,  5.5591e-01,  1.3719e-01, -9.8149e-01,  3.2648e-01,\n",
       "          -6.2384e-01, -1.7732e+00, -5.5528e-01, -1.4801e-01,  1.3730e+00],\n",
       "         [-6.7232e-01,  1.6555e+00,  1.2018e+00, -8.7744e-01,  1.4881e+00,\n",
       "          -6.1537e-01, -7.5571e-01, -1.1829e+00, -1.3013e-01, -1.1159e-01],\n",
       "         [-4.8765e-01,  1.6997e+00,  4.3939e-01, -1.7172e+00,  1.6613e-01,\n",
       "          -2.4763e-01, -6.4162e-01, -1.0511e+00,  1.3854e+00,  4.5466e-01]],\n",
       "\n",
       "        [[ 7.1202e-01, -5.8216e-01, -8.3667e-01, -5.3592e-01,  5.2411e-01,\n",
       "           1.7608e+00, -1.9000e+00,  1.0687e+00, -1.1665e-02, -1.9920e-01],\n",
       "         [-1.5072e+00, -6.4855e-01,  6.7126e-01,  7.3549e-01,  2.2925e+00,\n",
       "          -9.5898e-02, -3.2940e-01, -3.1190e-01,  1.0714e-01, -9.1338e-01],\n",
       "         [-8.7744e-01,  5.5755e-01, -6.5633e-01,  1.9946e-01, -7.6378e-01,\n",
       "          -1.2026e+00,  1.0691e+00,  2.2480e+00, -4.5656e-01, -1.1740e-01],\n",
       "         [ 7.0487e-01, -3.4483e-01, -4.8414e-01, -2.3812e+00,  1.1055e+00,\n",
       "           9.1520e-01, -7.8683e-01,  6.5620e-01,  6.0867e-01,  6.5461e-03],\n",
       "         [ 1.4793e+00, -9.4898e-01,  1.8660e+00, -8.9916e-02, -6.9975e-01,\n",
       "          -1.7476e-01,  7.4263e-01, -4.8461e-01, -2.5910e-01, -1.4308e+00]],\n",
       "\n",
       "        [[-1.7198e+00,  1.6942e+00, -4.0411e-01,  1.5460e+00, -7.2581e-01,\n",
       "           6.3338e-01, -2.8821e-01,  1.3826e-01, -7.5841e-01, -1.1549e-01],\n",
       "         [-1.1912e+00, -2.0640e-01,  5.0712e-01, -8.4375e-01, -1.5493e+00,\n",
       "          -4.0703e-01,  1.0963e+00,  7.9375e-01,  1.7807e+00,  1.9893e-02],\n",
       "         [-2.1472e+00,  1.9188e-01, -5.5853e-01, -3.9010e-01,  3.2862e-01,\n",
       "           1.3947e+00, -2.8236e-01,  1.0579e-01,  1.6334e+00, -2.7620e-01],\n",
       "         [ 9.6223e-01,  1.5981e-01,  1.1994e+00,  1.6814e+00, -5.4560e-01,\n",
       "           3.0701e-01, -9.5992e-01, -6.7571e-01, -4.5321e-01, -1.6755e+00],\n",
       "         [-9.2743e-01, -6.3628e-01,  9.7524e-01, -8.8747e-01,  2.4095e+00,\n",
       "          -2.7874e-01, -4.1978e-01, -7.0711e-01, -1.6730e-01,  6.3940e-01]],\n",
       "\n",
       "        [[-5.7024e-01,  8.7353e-01, -2.8092e-01,  2.2550e+00,  8.8650e-01,\n",
       "          -9.8249e-01,  1.8394e-01, -9.1304e-01, -5.3340e-01, -9.1890e-01],\n",
       "         [ 3.0572e-01,  1.5769e+00, -1.2559e-01,  1.3021e+00,  1.0135e-01,\n",
       "           3.4756e-02,  6.2044e-01, -1.1840e+00, -8.4283e-01, -1.7888e+00],\n",
       "         [ 2.4354e-01,  9.1677e-01, -1.2973e+00, -9.0264e-01, -2.5862e-01,\n",
       "           9.0989e-01,  6.7860e-01, -1.7928e-01, -1.6693e+00,  1.5583e+00],\n",
       "         [ 3.7157e-01,  3.3551e-01,  2.3462e-01,  1.6991e+00, -5.0403e-01,\n",
       "          -3.2588e-01, -2.2247e+00, -2.7074e-01, -4.2910e-01,  1.1137e+00],\n",
       "         [-1.0618e+00, -4.0671e-01,  4.9608e-01,  1.9713e+00, -1.4932e+00,\n",
       "          -1.0182e+00,  7.4365e-01,  1.5605e-01, -2.1482e-01,  8.2767e-01]],\n",
       "\n",
       "        [[ 1.2946e-01,  1.6771e+00,  1.5724e+00, -1.2934e+00, -7.8249e-02,\n",
       "           3.4526e-01, -1.2046e+00, -1.5129e-01,  1.8332e-01, -1.1799e+00],\n",
       "         [-2.2027e+00, -3.7971e-01,  5.2748e-01,  6.4967e-01,  2.3146e-01,\n",
       "           1.0518e+00,  2.0980e-01,  8.7646e-01, -1.4482e+00,  4.8396e-01],\n",
       "         [ 3.1535e-01, -6.9668e-01,  1.2688e-01,  5.2494e-01, -2.9194e-01,\n",
       "           9.9885e-01, -3.9209e-01, -1.1164e+00,  2.0682e+00, -1.5371e+00],\n",
       "         [-1.2089e+00, -1.3443e+00, -5.4530e-01,  6.8553e-01,  9.6846e-01,\n",
       "          -6.6439e-01, -7.6355e-01,  1.8307e+00,  2.8916e-01,  7.5255e-01],\n",
       "         [ 1.7560e+00,  1.2952e+00, -4.9780e-01,  1.4200e-01, -3.5407e-01,\n",
       "          -3.6196e-01,  1.8597e-01, -2.8452e-02, -2.1629e+00,  2.5985e-02]],\n",
       "\n",
       "        [[ 1.2749e-01,  6.1052e-01, -1.6774e+00,  2.1250e-01, -1.9723e-01,\n",
       "           1.8982e-02,  1.2259e+00, -1.6759e+00,  1.5387e+00, -1.8356e-01],\n",
       "         [-4.5962e-01, -1.0486e+00,  1.6704e+00, -6.0068e-01, -1.0854e+00,\n",
       "           1.3818e+00, -1.2695e+00,  3.0241e-01,  2.9766e-01,  8.1156e-01],\n",
       "         [ 2.2513e-01, -1.2804e+00,  6.9858e-01, -5.7452e-01,  9.6994e-01,\n",
       "          -7.9873e-01, -1.4294e+00,  2.2536e-01,  9.7017e-03,  1.9542e+00],\n",
       "         [-1.2424e+00,  1.5031e+00,  4.2651e-01,  1.1317e+00, -2.0737e+00,\n",
       "          -2.2404e-01, -3.0265e-01,  2.9153e-02,  3.1142e-01,  4.4087e-01],\n",
       "         [ 1.2979e+00, -2.5837e-01,  1.9076e-01,  4.4860e-01,  1.1196e+00,\n",
       "           4.3492e-01,  8.9127e-01, -1.1133e+00, -1.5031e+00, -1.5083e+00]],\n",
       "\n",
       "        [[-1.7319e+00, -6.3638e-01,  1.0642e+00, -8.9408e-01,  8.5645e-01,\n",
       "          -9.5086e-01,  5.0703e-01,  4.3972e-01, -2.4079e-01,  1.5866e+00],\n",
       "         [ 1.2566e+00, -9.4370e-01,  1.6647e+00,  6.8230e-01, -1.6430e+00,\n",
       "           1.5391e-01, -4.5920e-01, -1.9369e-01,  5.1164e-01, -1.0297e+00],\n",
       "         [ 1.2643e+00, -7.5465e-01,  4.2966e-01,  1.4378e+00,  7.1162e-01,\n",
       "          -1.4878e+00,  2.9973e-01,  2.9071e-01, -7.1840e-01, -1.4729e+00],\n",
       "         [ 3.9522e-01, -2.3069e+00,  6.7931e-01,  2.5330e-01,  1.3906e+00,\n",
       "          -8.9417e-01,  8.0514e-01,  3.1813e-01, -7.1276e-01,  7.2149e-02],\n",
       "         [-1.6468e+00, -1.0105e+00, -1.5521e+00,  3.0555e-01,  9.5578e-01,\n",
       "           3.7671e-01,  3.9198e-01,  1.4876e+00,  5.7283e-01,  1.1908e-01]],\n",
       "\n",
       "        [[-9.9579e-01,  1.0579e+00, -1.3524e-01,  5.8171e-01, -3.6859e-02,\n",
       "           2.8086e-01,  1.6351e-01, -2.4079e+00,  2.3842e-01,  1.2533e+00],\n",
       "         [ 8.9567e-01, -1.5506e-01, -1.1492e+00,  1.9909e+00, -1.5779e-01,\n",
       "           7.4010e-01, -1.5668e+00, -9.0144e-01,  1.8994e-01,  1.1370e-01],\n",
       "         [ 5.3581e-01,  1.6448e-01,  6.7743e-01, -5.6972e-01, -4.5641e-01,\n",
       "           1.6302e+00, -1.9997e+00,  1.1567e+00, -7.2967e-01, -4.0902e-01],\n",
       "         [-1.1835e+00, -1.0615e+00,  2.3020e-01,  8.7525e-01,  1.0786e+00,\n",
       "           1.2364e+00, -3.1946e-01, -6.7475e-01,  1.2107e+00, -1.3921e+00],\n",
       "         [ 7.1756e-01, -1.2025e-01,  2.2664e+00, -1.0574e+00, -4.6349e-01,\n",
       "           3.1481e-01,  3.3861e-01, -5.9660e-01, -1.5518e+00,  1.5226e-01]],\n",
       "\n",
       "        [[-1.0683e+00,  3.5255e-01,  1.6310e+00,  8.2657e-01, -1.4485e-01,\n",
       "           1.3847e+00, -8.8420e-01,  1.7474e-01, -9.6479e-01, -1.3074e+00],\n",
       "         [-1.8778e+00,  1.1526e+00, -6.6493e-01, -4.4828e-01, -5.4742e-01,\n",
       "           6.6383e-02,  1.8051e+00,  7.8751e-01,  2.3918e-01, -5.1236e-01],\n",
       "         [ 6.8375e-01, -8.3493e-01,  9.6132e-01,  7.2061e-01, -1.1372e+00,\n",
       "           5.3764e-02, -1.9444e+00,  1.5081e+00,  1.3664e-01, -1.4761e-01],\n",
       "         [-3.1913e-01, -6.9192e-02,  1.3162e+00, -1.1987e+00, -1.1679e+00,\n",
       "          -3.1970e-01,  1.1060e+00, -3.4092e-01, -8.1196e-01,  1.8053e+00],\n",
       "         [ 1.7985e+00, -1.2012e-01, -3.4836e-01,  6.8339e-01, -1.1236e+00,\n",
       "          -1.5494e+00,  1.9309e-01, -8.1935e-01, -5.1321e-02,  1.3371e+00]],\n",
       "\n",
       "        [[ 2.0645e-01, -7.4471e-01,  1.4276e+00,  1.5004e-01,  1.0482e+00,\n",
       "          -3.2470e-01,  1.1365e-01, -1.1444e+00, -1.8739e+00,  1.1417e+00],\n",
       "         [-1.1829e+00, -2.0352e+00, -8.4639e-02,  4.3246e-01,  7.8507e-01,\n",
       "           1.2526e+00,  6.5561e-01, -9.5494e-01,  3.4581e-01,  7.8606e-01],\n",
       "         [ 1.0933e+00, -4.6438e-01,  2.1436e+00, -1.2200e+00, -4.5681e-01,\n",
       "           5.3236e-02, -4.9603e-02, -1.5127e-02, -1.4650e+00,  3.8076e-01],\n",
       "         [ 4.8817e-01, -1.9065e+00, -7.1201e-01,  7.9141e-01, -8.9704e-01,\n",
       "          -3.8160e-02, -5.2148e-01,  1.7649e+00,  8.8244e-01,  1.4826e-01],\n",
       "         [-4.5574e-01,  1.1132e+00, -3.5105e-01, -5.6414e-01,  7.5440e-01,\n",
       "          -8.8273e-01,  1.4473e+00, -6.0678e-01, -1.6759e+00,  1.2214e+00]],\n",
       "\n",
       "        [[-2.4101e-01,  1.0015e-01, -2.0733e+00,  1.2535e+00,  9.4423e-01,\n",
       "           4.2544e-01, -4.6579e-01,  6.7744e-01, -1.3400e+00,  7.1937e-01],\n",
       "         [-2.1565e-01,  6.2744e-01,  6.1977e-02,  1.6163e+00,  1.0984e+00,\n",
       "           3.2547e-01, -2.1089e-01, -2.2112e+00, -7.7060e-01, -3.2135e-01],\n",
       "         [ 2.7782e+00,  5.2705e-02, -6.7169e-01, -2.0228e-02, -4.2798e-01,\n",
       "           2.7488e-01, -4.5952e-02, -3.8572e-01, -4.5306e-01, -1.1011e+00],\n",
       "         [-1.0879e-01, -7.8689e-02,  4.7857e-01, -2.5478e-01, -1.6591e+00,\n",
       "           3.7414e-01, -1.2578e+00,  1.2305e+00,  1.8390e+00, -5.6303e-01],\n",
       "         [-1.2974e-01,  6.4286e-02, -8.0751e-01,  5.7064e-01,  8.6139e-01,\n",
       "          -1.5502e+00,  2.0957e+00,  2.8593e-01, -2.3943e-01, -1.1511e+00]],\n",
       "\n",
       "        [[-2.5670e-01, -1.3663e+00,  2.1492e+00,  4.2185e-01, -3.7135e-01,\n",
       "           2.9024e-01,  1.5545e-01,  9.2403e-01, -6.0192e-01, -1.3445e+00],\n",
       "         [-5.8705e-01,  5.6149e-01,  6.7020e-01,  8.3896e-01,  5.0075e-01,\n",
       "           9.4598e-01, -2.0864e+00, -4.5593e-01,  9.0261e-01, -1.2906e+00],\n",
       "         [-2.2451e-01,  8.7647e-01, -1.0268e+00, -5.4766e-01, -2.7925e-01,\n",
       "          -6.0199e-01, -1.7639e+00,  1.0424e+00,  1.2487e+00,  1.2765e+00],\n",
       "         [-2.9807e-01,  1.0149e+00,  1.9953e+00, -2.5345e-01, -6.8644e-01,\n",
       "          -1.8816e+00, -6.0604e-01, -1.7300e-01,  3.1706e-01,  5.7131e-01],\n",
       "         [ 2.9555e-01, -2.0001e+00, -8.5448e-01,  7.6445e-01,  5.9596e-01,\n",
       "           1.1638e+00, -5.2313e-01, -2.5541e-01,  1.4599e+00, -6.4644e-01]],\n",
       "\n",
       "        [[-7.4677e-01, -1.2935e+00,  5.8821e-01,  1.7553e+00,  2.4100e-01,\n",
       "           9.6333e-01,  9.3956e-01, -1.1695e+00, -2.5868e-01, -1.0190e+00],\n",
       "         [ 4.9494e-01,  2.2479e-01,  2.2206e-01, -8.7570e-01, -8.2803e-01,\n",
       "           9.7594e-01, -5.8020e-01, -1.2086e+00, -6.6393e-01,  2.2387e+00],\n",
       "         [-5.5364e-01,  6.4272e-01, -9.0943e-01,  1.6243e+00,  1.7551e+00,\n",
       "          -1.8566e-01, -3.6039e-01,  1.6885e-01, -1.3753e+00, -8.0653e-01],\n",
       "         [-4.4110e-01,  1.0370e+00, -1.3073e-01,  1.1841e+00, -1.1868e+00,\n",
       "           1.9224e+00, -1.2917e-02, -1.0012e+00, -1.0492e+00, -3.2155e-01],\n",
       "         [ 1.0278e+00, -4.7854e-01,  1.3485e+00, -3.5367e-01,  1.6454e+00,\n",
       "          -1.3661e+00, -1.4305e+00, -2.7896e-01,  1.2596e-01, -2.3993e-01]],\n",
       "\n",
       "        [[-9.4771e-01, -4.7570e-01,  1.4337e+00,  1.6028e+00, -1.7333e+00,\n",
       "           6.9494e-01,  2.8297e-01,  1.5649e-01, -2.3782e-01, -7.7637e-01],\n",
       "         [ 8.8795e-01, -6.1132e-01,  5.3829e-01, -1.8577e-01,  1.0243e+00,\n",
       "          -9.0397e-01, -1.8276e+00, -1.0229e+00,  8.8721e-01,  1.2139e+00],\n",
       "         [ 1.3558e+00,  1.7353e+00, -1.1819e+00, -7.7132e-01, -1.2822e+00,\n",
       "          -5.0791e-01,  6.7655e-01, -7.3637e-01,  3.9510e-01,  3.1685e-01],\n",
       "         [-1.0182e+00, -6.8837e-01, -3.9586e-01,  4.4416e-02, -3.9528e-01,\n",
       "           2.1123e-01,  2.3068e+00,  1.3166e+00, -9.3641e-01, -4.4492e-01],\n",
       "         [-4.6252e-01,  7.3295e-01,  8.8843e-02, -1.0244e+00,  1.9651e+00,\n",
       "          -1.9841e+00,  5.1794e-01, -2.0765e-01,  1.1099e-01,  2.6291e-01]],\n",
       "\n",
       "        [[ 6.9193e-01,  8.6696e-01,  1.4009e-01, -3.5297e-01, -3.1923e-01,\n",
       "          -1.0478e+00, -2.0776e+00,  1.5891e+00,  7.3124e-01, -2.2177e-01],\n",
       "         [-2.1359e+00,  6.4292e-01,  5.2353e-01,  4.6018e-01, -5.7426e-01,\n",
       "           1.4646e+00,  4.4023e-02,  8.8414e-01, -1.1146e+00, -1.9472e-01],\n",
       "         [ 9.8424e-01, -1.7925e-01,  2.4338e-01, -1.2733e+00,  6.1414e-01,\n",
       "          -7.8698e-01,  8.1876e-01,  1.2125e+00, -2.0099e+00,  3.7638e-01],\n",
       "         [ 4.7607e-01, -7.0320e-01, -1.9101e-01,  1.2106e+00, -5.3718e-01,\n",
       "          -1.2821e+00,  9.9134e-01,  1.5483e+00, -1.5689e+00,  5.6045e-02],\n",
       "         [-1.5802e+00,  8.0956e-01, -1.2051e+00,  1.8517e+00,  9.1741e-01,\n",
       "          -5.8573e-01,  2.7038e-02, -7.4939e-01,  4.8960e-02,  4.6574e-01]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP Example\n",
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "print(embedding.mean(-1, keepdim=True).shape)\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "# Activate module\n",
    "# layer_norm(embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45305027dd5e5f2c6dc87ef688e0ef3331115f246ec5e2f248ae43438a3fdc70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
