{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制\n",
    "注意力机制就是建模相似度  \n",
    "```\n",
    "mapping a query and a set of key-value pairs to an output\n",
    "```\n",
    "**output** computed as a weighted sum of the values  \n",
    "**weight** compatibility between query and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导个包先\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention is all you need\n",
    "transformer架构的开山之作 最开始用在神经网络翻译领域  \n",
    "\n",
    "#### 相关概念介绍\n",
    "`embedding`: 把输入编码成等长的向量 从现实空间 -> 向量空间   \n",
    "`Q for query`: 在这里任务里面 Q就是一句话的embedding n个长为d的向量 `[n_q, d_q]`   \n",
    "`KV for key and value`: 我们这里是计算自注意力 所以key和value也是由同样的输入算出来的   \n",
    "\n",
    "#### 网络结构\n",
    "![](../../Source/transformer.png)\n",
    "\n",
    "#### 注意力公式\n",
    "$\n",
    "attention = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$\n",
    "\n",
    "#### 流程分解\n",
    "1. matmul  U = Q @ K^T\n",
    "2. scale   U = U / sqrt(d_k)\n",
    "3. mask    U = U.masked_fill(mask, -inf)\n",
    "4. softmax A = softmax(U)\n",
    "5. matmul  O = A @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试代码\n",
    "batch = 1\n",
    "n_q, n_k, n_v = 2, 4, 4\n",
    "d_q, d_k, d_v = 128, 128, 64\n",
    "\n",
    "q = torch.randn(batch, n_q, d_q)\n",
    "k = torch.randn(batch, n_k, d_k)\n",
    "v = torch.randn(batch, n_v, d_v)\n",
    "mask = torch.zeros(batch, n_q, n_k).bool()\n",
    "\n",
    "attention = Attention()\n",
    "attn, output = attention(q, k, v, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        B, n_k, d_k = k.shape()\n",
    "        u = torch.bmm(q, k.transpose(1, 2))\n",
    "        u /= torch.sqrt(d_k)\n",
    "        attn = F.softmax(u, dim=2)\n",
    "\n",
    "        return torch.bmm(attn, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim=768, nums_heads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_Q = nn.Linear(dim, d_q)\n",
    "        self.W_K = nn.Linear(dim, d_k)\n",
    "        self.W_V = nn.Linear(dim, d_v)\n",
    "        self.fc = nn.Linear(d_v, dim)\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "\n",
    "        self.nums_heads = nums_heads\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        residual, batch = q, q.shape(0)\n",
    "\n",
    "        q = self.W_Q(q).reshape(batch, n_q, self.nums_heads, -1).transpose(1, 2)\n",
    "        k = self.W_K(k).reshape(batch, n_k, self.nums_heads, -1).transpose(1, 2)\n",
    "        V = self.W_K(v).reshape(batch, n_v, self.nums_heads, -1).transpose(1, 2)\n",
    "\n",
    "        attn = torch.bmm(q, k.transpose(2, 3))  # [bacth, heads, n_q, n_k]\n",
    "        attn = F.softmax(attn / np.sqrt(d_k), dim=-1)\n",
    "        output = torch.bmm(attn, v).transpose(1, 2).reshape(batch, n_v, -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return self.ln(output + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(in_features=dim, out_features=dim * 3)\n",
    "        self.proj = nn.Linear(in_features=dim, out_features=dim)\n",
    "        self.num_heads = num_heads\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batchsz, heads, nums, dim//heads]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * np.power(C, -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An image is worth 16×16 words\n",
    "Vision Transformer 开山之作 把transformer引入到了CV领域\n",
    "\n",
    "#### 相关概念介绍\n",
    "这里只是把单词换成了16×16的图像块 一张图像作为一个句子来处理\n",
    "\n",
    "#### 注意力公式\n",
    "$\n",
    "attention = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$\n",
    "\n",
    "#### 流程分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "B = 1    # batch size\n",
    "N = 196  # patches\n",
    "C = 768  # channel / embed_dim\n",
    "\n",
    "image = torch.randn([1, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PatchEmbed实际上利用kernel_size和stride都等于patch_size的二维卷积将图像分块  \n",
    "```python\n",
    "                [1, 3, 224, 224]\n",
    "Conv2d      ->  [1, 768, 14, 14]\n",
    "flatten     ->  [1, 768, 196]\n",
    "transpose   ->  [1, 196, 768]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels=in_c, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "patchembed = PatchEmbed()\n",
    "image = patchembed(image)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拆分成多头计算注意力然后拼起来, patch_num应该是196+1, 有一个cls_token  \n",
    "一个线性层计算出qkv, 然后就是常规的注意力操作\n",
    "```python\n",
    "                [1, 196, 768]\n",
    "linear      ->  [1, 196, 3, 8, 96]\n",
    "permute     ->  [3, 1, 8, 196, 96]\n",
    "q @ k       ->  [1, 8, 196, 196]\n",
    "attn @ v    ->  [1, 8, 196, 96]\n",
    "transpose   ->  [1, 196, 8, 96]\n",
    "reshape     ->  [1, 196, 768]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(in_features=dim, out_features=dim * 3)\n",
    "        self.proj = nn.Linear(in_features=dim, out_features=dim)\n",
    "        self.num_heads = num_heads\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        # [batchsz, blocks, 3, heads, dim//heads] - > [3, batchsz, heads, blocks, dim//heads]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(1, 3, 0, 2, 4)\n",
    "        # [batchsz, heads, blocks, dim//heads]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * np.power(C, -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "attention = Attention(dim=C)\n",
    "image = attention(image)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim * 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.fc(x)\n",
    "\n",
    "        return self.ln(output + residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batchnorm是在一个batch的各个样本之间做归一化，参数大小是 2×C  \n",
    "Layernorm则是在特征之间做归一化，所以参数大小和batchsize有关？\n",
    "\n",
    "可以看看这个，LayerNorm就是加在最后D个维度上，被称为normalized_shape  \n",
    "https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
    "\n",
    "为什么对于图像是`nn.LayerNorm([C, H, W])`而对于NLP是`nn.LayerNorm(embedding_dim)`  \n",
    "考虑单独的实例，一张图像的组成就是[C, H, W]， 而一个单词只是[N]维的向量，这里做LN是在单词维度上，而不是句子，所以只需要最后一个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-12) -> None:\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.parameter(torch.zeros(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 是把batch内每个实例，单独对其特征求均值和方差，这里的特征只有最后一个维度\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True)\n",
    "\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)  # 尺度不变\n",
    "        out = self.gamma * out + self.beta  # 特征变换 所以gamma和beta是dim尺寸\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_dim, dropout_rate=0.1):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads)\n",
    "        self.mlp = MLP(dim, mlp_dim, dropout_rate)\n",
    "        self.drop_path_prob = dropout_rate  # Drop Path 的概率\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 自注意力层\n",
    "        attention_output = self.attn(self.norm1(x))\n",
    "        # 添加 drop path 或者残差连接\n",
    "        x = x + self.drop_path(attention_output)\n",
    "        \n",
    "        # 多层感知器层\n",
    "        mlp_output = self.mlp(self.norm2(x))\n",
    "        # 再次添加 drop path 或者残差连接\n",
    "        x = x + self.drop_path(mlp_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def drop_path(self, x):\n",
    "        if self.training and self.drop_path_prob > 0:\n",
    "            keep_prob = 1.0 - self.drop_path_prob\n",
    "            mask = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < keep_prob\n",
    "            x = x / keep_prob * mask  # 对保留的路径进行缩放\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, embed_dim, depth, num_heads):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        # 计算图像中的分块数量\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # 创建图像的分块嵌入\n",
    "        self.patch_embed = PatchEmbed(image_size=image_size, patch_size=patch_size, embed_dim=embed_dim)\n",
    "\n",
    "        # 类别编码\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        # 位置编码\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    "\n",
    "        # Transformer 编码块的堆叠\n",
    "        self.blocks = nn.Sequential(*[Block(embed_dim=embed_dim, num_heads=num_heads, mlp_dim=4 * embed_dim, depth=depth) for _ in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 图像分块嵌入\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # 扩展类别编码\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "\n",
    "        # 连接类别编码和分块嵌入\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        # 加上位置编码\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # 通过 Transformer 编码块\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # 返回类别编码\n",
    "        return x[:, 0]\n",
    "\n",
    "# 创建 VisionTransformer 模型的实例\n",
    "image_size = 224\n",
    "patch_size = 16\n",
    "num_classes = 1000\n",
    "embed_dim = 768\n",
    "depth = 12\n",
    "num_heads = 12\n",
    "model = VisionTransformer(image_size, patch_size, num_classes, embed_dim, depth, num_heads)\n",
    "\n",
    "# 示例输入\n",
    "input_data = torch.randn(1, 3, image_size, image_size)\n",
    "\n",
    "# 使用模型进行前向传播\n",
    "output = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "cls_token = torch.nn.Parameter(torch.zeros(1, 1, 768))\n",
    "cls_token = cls_token.expand(10, -1, -1)\n",
    "cls_token.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
